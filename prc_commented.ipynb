{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKUjWY6iCEAqA3QJ2g4SKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zaitsevIV/hse_gp2_scrap-api/blob/develop/prc_commented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGmuhRqAMGT_",
        "outputId": "2300ab70-824a-4eb2-cc95-439425dfbfee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49/49 [16:51<00:00, 20.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Время загрузки данных по Республики Башкортостан - 16.861239620049794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "links_basck = [] #сохраняем ссылки на вакансии\n",
        "pages = range(1, 50) #сохраняем номера страниц\n",
        "time_start = time.time() #фиксируем старт скрапинга\n",
        "for current_page in tqdm(pages): #циклом проходимся по странице и забираем ссылки на вакансии\n",
        "  url = f'https://bashkortostan.rabota.ru/vacancy/?page={current_page}'\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.text, 'html')\n",
        "  for link in soup.find_all('a', class_=\"vacancy-preview-card__title_border\"): #отбираем элемент, в котором есть редирект на вакансию\n",
        "    links_basck.append(f\"https://bashkortostan.rabota.ru{link.get('href')}\") #сохраняем получившуюся ссылку\n",
        "    time.sleep(0.5) #Небольшая пауза, чтобы остаться скрытными\n",
        "  time.sleep(2) #Небольшая пауза, чтобы остаться скрытными\n",
        "time_end = time.time() #фиксируем конец скрапинга\n",
        "print(f'\\nВремя загрузки данных по Республики Башкортостан - {(time_end - time_start) / 60}') #выводим время груза"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "links_chelyb = [] #сохраняем ссылки на вакансии\n",
        "pages = range(1, 54) #сохраняем номера страниц\n",
        "time_start = time.time() #фиксируем старт скрапинга\n",
        "for current_page in tqdm(pages): #циклом проходимся по странице и забираем ссылки на вакансии\n",
        "  url = f'https://chelyabinskaya.rabota.ru/vacancy/?page={current_page}'\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.text, 'html')\n",
        "  for link in soup.find_all('a', class_=\"vacancy-preview-card__title_border\"): #отбираем элемент, в котором есть редирект на вакансию\n",
        "    links_chelyb.append(f\"https://chelyabinskaya.rabota.ru{link.get('href')}\") #сохраняем получившуюся ссылку\n",
        "    time.sleep(0.5) #Небольшая пауза, чтобы остаться скрытными\n",
        "  time.sleep(2) #Небольшая пауза, чтобы остаться скрытными\n",
        "time_end = time.time() #фиксируем конец скрапинга\n",
        "print(f'\\nВремя загрузки данных по Челябинской области {(time_end - time_start) / 60}') #выводим время груза"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B78nDK_lkueH",
        "outputId": "5ad635b2-98de-4ab2-f5fa-ab7392dc9c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 53/53 [18:22<00:00, 20.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Время загрузки данных по Челябинской области 18.37250917752584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_basck = [] #создаем массив для итоговых результатов скрапа\n",
        "time_start = time.time() #фиксируем старт обработки вакансий\n",
        "for i in tqdm(range(len(links_basck))): #циклом проходимся по вакансиям и забираем значения на вакансии\n",
        "  page = requests.get(links_basck[i])\n",
        "  soup = BeautifulSoup(page.text, 'html')\n",
        "  try: #так как в некоторых вакансиях могут быть не указаны данные, делаем проверку на наличие\n",
        "    city = soup.find('span', class_='vacancy-requirements__city').get_text().strip()[:-1] #Вытаскиваем значение города и обрезаем так, чтобы не было лишнего\n",
        "  except AttributeError:\n",
        "    city = np.nan # так как не нашли - оставляем пустое значение\n",
        "  try: #так как в некоторых вакансиях могут быть не указаны данные, делаем проверку на наличие\n",
        "    position = soup.find('h1', class_='vacancy-card__title').get_text().replace(\"\\n\", \"\").strip() #Вытаскиваем значение позиции и обрезаем так, чтобы не было лишнего\n",
        "  except AttributeError:\n",
        "    position = np.nan # так как не нашли - оставляем пустое значение\n",
        "  try: #так как в некоторых вакансиях могут быть не указаны данные, делаем проверку на наличие\n",
        "    salary = soup.find('h3', class_='vacancy-card__salary').get_text() #Вытаскиваем значение зарплаты\n",
        "  except AttributeError:\n",
        "    salary = np.nan # так как не нашли - оставляем пустое значение\n",
        "  result_basck.append([city, position, salary, links_basck[i]]) #Сохраняем данные по вакансиям\n",
        "  time.sleep(0.5) #Небольшая пауза, чтобы остаться скрытными\n",
        "time_end = time.time() #фиксируем конец скрапинга"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L3L3AczbyIS",
        "outputId": "43ae29a3-c0d9-4b38-aa1e-5be7647045c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1459/1459 [59:16<00:00,  2.44s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_chelyb = [] #создаем массив для итоговых результатов скрапа\n",
        "time_start = time.time() #фиксируем старт обработки вакансий\n",
        "for i in tqdm(range(len(links_chelyb))): #циклом проходимся по вакансиям и забираем значения на вакансии\n",
        "  page = requests.get(links_chelyb[i])\n",
        "  soup = BeautifulSoup(page.text, 'html')\n",
        "  try: #так как в некоторых вакансиях могут быть не указаны данные, делаем проверку на наличие\n",
        "    city = soup.find('span', class_='vacancy-requirements__city').get_text().strip()[:-1] #Вытаскиваем значение города и обрезаем так, чтобы не было лишнего\n",
        "  except AttributeError:\n",
        "    city = np.nan # так как не нашли - оставляем пустое значение\n",
        "  try: #так как в некоторых вакансиях могут быть не указаны данные, делаем проверку на наличие\n",
        "    position = soup.find('h1', class_='vacancy-card__title').get_text().replace(\"\\n\", \"\").strip() #Вытаскиваем значение города и обрезаем так, чтобы не было лишнего\n",
        "  except AttributeError:\n",
        "    position = np.nan # так как не нашли - оставляем пустое значение\n",
        "  try: #так как в некоторых вакансиях могут быть не указаны данные, делаем проверку на наличие\n",
        "    salary = soup.find('h3', class_='vacancy-card__salary').get_text() #Вытаскиваем значение зарплаты\n",
        "  except AttributeError:\n",
        "    salary = np.nan # так как не нашли - оставляем пустое значение\n",
        "  result_chelyb.append([city, position, salary, links_chelyb[i]]) #Сохраняем данные по вакансиям\n",
        "  time.sleep(0.5) #Небольшая пауза, чтобы остаться скрытными\n",
        "time_end = time.time() #фиксируем конец скрапинга"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5opDkHvl0iY",
        "outputId": "960ed403-5048-4d53-ff53-d938f6fe7c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1585/1585 [1:04:56<00:00,  2.46s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "chelyb = pd.DataFrame(result_chelyb, columns=['city', 'position', 'salary', 'link']) #Создаем дф для хранения данных\n",
        "basck = pd.DataFrame(result_basck, columns=['city', 'position', 'salary', 'link']) #Создаем дф для хранения данных"
      ],
      "metadata": {
        "id": "5NXI0DAlbUZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basck.to_csv('basck', sep=',', encoding='utf-8') #Создаем csv, чтобы не перезапускать весь процесс\n",
        "chelyb.to_csv('chelyb', sep=',', encoding='utf-8') #Создаем csv, чтобы не перезапускать весь процесс"
      ],
      "metadata": {
        "id": "ILBbyl54c2uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "basck = pd.read_csv('basck.csv')"
      ],
      "metadata": {
        "id": "w_HJUbzuH57U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}